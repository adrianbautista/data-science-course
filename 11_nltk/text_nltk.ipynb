{
 "metadata": {
  "name": "",
  "signature": "sha256:a98836a0115dc7b9aa049992db3d0d64b7b50d31083bc9a42353579fcc93ef76"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# REST, JSON, and text\n",
      "\n",
      "## Class Objectives:\n",
      "\n",
      "* Understand what REST and JSON are\n",
      "* implementation of the `json` module in python\n",
      "* Reviewing some of the useful functionality in `nltk`\n",
      "* Reviewing concepts of text handling (counting, stopwords, corpi and dictionaries)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## To Start:\n",
      "\n",
      "We'll talk about the difficulty of text parsing by looking at this website first:\n",
      "\n",
      "http://translationparty.com/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## REST and JSON\n",
      "\n",
      "REST stands for Representational State Transfer. It's a simplification of architecture for networked applications. The basics found in Rails and Django apps insist on a RESTful framework for its ease of use compared to other architectures, such as CORBA, or [SOAP](https://developers.google.com/adwords/api/docs/guides/soap).\n",
      "\n",
      "RESTful applications do four primary things around \"resources:\"\n",
      "\n",
      "* `GET`: retrieve the collection of data\n",
      "* `PUT`: Replace or update the collection of data\n",
      "* `POST`: Create a new collection of data\n",
      "* `DELETE`: Delete the collection of data\n",
      "\n",
      "It's also the premise for [AJAX](http://api.jquery.com/jquery.ajax/) calls, and primarily how web services interact between your client (your computer) and the web server (where the web application resides). These AJAX calls primarily use JSON to send data between the server and client.\n",
      "\n",
      "JSON stands for JavaScript Object Notation. For Python, it essentially looks like a stringified version of a dictionary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "valid_json = '{ \"some_kinda_key\": \"some_kinda_value\" }'\n",
      "\n",
      "also_valid_json = '[{ \"some_kinda_key\": \"some_kinda_value\" }, { \"some_other_key\": \"some_other_value\" }]'\n",
      "\n",
      "actually_a_dictionary = {'some_kinda_key': 'some_kinda_value'}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since JSON uses a hash/dictionary-like format, it is traditionally unstructured, compared to CSV files, though when interacting with APIs (Application Programming Interface), if you're `GET`ing a series of the same collection, generally, the data will look the same. A typical JSON request would look something like this:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```javascript\n",
      "{\n",
      "    \"response\": 200,\n",
      "    \"params\": {\n",
      "        \"q\": \"dinosaurs\",\n",
      "        \"count\": 50\n",
      "    },\n",
      "    \"results\": [{ ... }]\n",
      "}    \n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which likely was sent from a url request that looks like this:\n",
      "\n",
      "```\n",
      "https://api.awebsite.com/v1/search/?q=dinosuars&count=50\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To load JSON into python, we use the `json` module. To find JSON, we'll dig it out of a network transfer from a website (petfinder will do, in this case)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import json\n",
      "\n",
      "pets = json.load(open('petfinder.json'))\n",
      "\n",
      "print type(pets)\n",
      "print pets.keys()\n",
      "print type(pets['results'])\n",
      "print len(pets['results'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "<type 'dict'>\n",
        "[u'meta_data', u'results', u'links']\n",
        "<type 'list'>\n",
        "15\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In the above JSON, it looks like petfinder by default returns a list of 15 animals.\n",
      "\n",
      "Petfinder also has an API, so you can get an API key and build requests using the `requests` package in python. You should get similar results."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Your Turn: JSON \n",
      "\n",
      "\n",
      "1. Describe the data that exists in each primary key from the json (meta_data, results, links).\n",
      "2. Since the results key is a list (of animals), we could effectively turn that into a pandas DataFrame. What's the python implementation to do that? What would be an effective index to set?\n",
      "3. Could we write a function that reads in json and creates the data frame for you? (imagine the url structure below, and note that the data structure this url returns is quite different from what we grabbed in class)\n",
      "\n",
      "`http://api.petfinder.com/pet.find?key=98719f8ded45b41f3153f5736d55d162&location=10003&format=json`\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Libraries that do the work for us\n",
      "\n",
      "Building url requests, while not incredibly complicated (usually), are still a task. You could write a python library to do the work for you [automagically](https://github.com/gtaylor/petfinder-api).\n",
      "\n",
      "Since we will be working with text data, a good location for live text data is from [twitter](https://github.com/bear/python-twitter). We can disect the code below:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "```py\n",
      "# coding=utf-8\n",
      "from twitter import *\n",
      "\"\"\"\n",
      "there would be code here to set up a dictionary including everything required to load the authentication for twitter.\n",
      "What are the advantages of setting up a dictionary here instead of just defining it below?\n",
      "\"\"\"\n",
      "t = Twitter(\n",
      "    auth=OAuth(\n",
      "        token=config['TOKEN'],\n",
      "        token_secret=config['TOKEN_SECRET'],\n",
      "        consumer_key=config['CONSUMER_KEY'],\n",
      "        consumer_secret=config['CONSUMER_SECRET'])\n",
      "    )\n",
      "\n",
      "hashtags=[\n",
      "    u'hongkong',\n",
      "    u'occupycentral',\n",
      "    u'umbrellarevolution',\n",
      "    u'china',\n",
      "    u'hk',\n",
      "    u'admiralty',\n",
      "    u'occupyhk',\n",
      "]\n",
      "\n",
      "all_results = []\n",
      "for hashtag in hashtags:\n",
      "    results = t.search.tweets(q='#'+hashtag, count=100, result_type='mixed')\n",
      "    for r in results['statuses']:\n",
      "        try:\n",
      "            clean_tweet = unicode(r['text']).encode('utf-8').replace('\\n', ' ').replace('\\r', ' ').replace('\"', \"'\")\n",
      "            print u','.join([unicode(r['id']),'\"'+clean_tweet+'\"', '\"'+hashtag+'\"'])\n",
      "        except UnicodeDecodeError:\n",
      "            pass\n",
      "```"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Your turn: API\n",
      "\n",
      "1. Read through the [search API](https://dev.twitter.com/rest/reference/get/search/tweets). What are the additional parameters that can be set?\n",
      "2. How could the script be changed above to be regularly used and make sure you're getting distinct tweets each time it runs\n",
      "3. Experimental design: Given the additional parameters that could be set, and the above code, what could be some interesting questions to ask and then explore? Ex: \"Comparing tweets about HK in Hong Kong vs Beijing\"\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Working with text and using NLTK"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`nltk` stands for natural language toolkit. It's primary purpose in our data science toolkit sits around the following functionality:\n",
      "\n",
      "* parsing text objects into lists of tokens\n",
      "* providing context and similarity between text\n",
      "* containers for large amounts of text (various texts from project gutenberg are included for download)\n",
      "* tagging words\n",
      "* building predictive models using text"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Below we'll load in some previously generated twitter data into a pandas dataframe, though with some integration, it wouldn't be a stretch to stream them directly into a pandas data frame, or into a SQL database, which we could later pull in with pandas."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "tweets = pd.read_csv('twitter5.csv')\n",
      "tweets = tweets\n",
      "\n",
      "print tweets"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                      id                                              tweet  \\\n",
        "0     522205943074160640  Is this #HongKong 's Rodney King? Police need ...   \n",
        "1     521669229188501504  'We won't move and I'm ready to get arrested',...   \n",
        "2     522228472786067456  RT @stanyee: Footage of beating prompts #HongK...   \n",
        "3     522228386002108418  What is happening in Hong Kong is something th...   \n",
        "4     522228373964480512  #Funding:#HongKong #travel #startup @KlookTrav...   \n",
        "5     522228351231344640  HK police use pepper spray on protesters, beat...   \n",
        "6     522228250450206720  RT @stanyee: Footage of beating prompts #HongK...   \n",
        "7     522228232330817536  RT @stanyee: Footage of beating prompts #HongK...   \n",
        "8     522228119952822272  Squeezed. #vscocam #vsco_hub #vscogang #vscogr...   \n",
        "9     522228020069679104  RT @stanyee: Footage of beating prompts #HongK...   \n",
        "10    522227916659105792  BREAKING: #HongKong security chief says 6 poli...   \n",
        "11    522227892198334464  Trendingnews: #hongkong #demonstranten | #aang...   \n",
        "12    522227878755184642  @BBCBreaking In #HongKong #UmbrellaMovement, p...   \n",
        "13    522227788095303680  @cnnbrk In #HongKong #UmbrellaMovement, police...   \n",
        "14    522227771502653440  Map of the underpass in #hongkong where police...   \n",
        "15    522227564555673601  @nytimes In #HongKong #UmbrellaMovement, polic...   \n",
        "16    522227525598994432  RT @CoconutsHK: 37 men and 8 women arrested la...   \n",
        "17    522227458012368897             #Hongkong today http://t.co/QWD1odyMdS   \n",
        "18    522227399069802496  RT @SauloCorona: ICYMI: #HongKong tensions ris...   \n",
        "19    522227204743524353  RT @stanyee: Footage of beating prompts #HongK...   \n",
        "20    522227036161839105  Los 7 goles que le hicimos a #HongKong son los...   \n",
        "21    522227008042840064  #EXO #hongkong #FROMLOSTPLANET  #20140601 #cha...   \n",
        "22    522227000887361536  @WSJAsia @WSJ This was how the #HongKong polic...   \n",
        "23    522226702475218944  Statement by #HongKong Police about the video ...   \n",
        "24    522226690244628482  #hongkong Oil prices rebound... but not for lo...   \n",
        "25    522226686138388480  #hongkong Oil prices rebound... but not for lo...   \n",
        "26    522226684573933568  #hongkong Oil prices rebound... but not for lo...   \n",
        "27    522226680367034369  #HongKong RT @george_chen: First tear gas, no ...   \n",
        "28    522226675892092928  Footage of beating prompts #HongKong police to...   \n",
        "29    522226672771530754  the underpass in #hongkong where police used p...   \n",
        "...                  ...                                                ...   \n",
        "1466  522495130905763841  RT @krislc: Legislator Leung Kwok-hung aka Lon...   \n",
        "1467  522495102971699200  RT @krislc: Audrey Eu giving out free eggtarts...   \n",
        "1468  522494858507071488  RT @tomgrundy: Big Pictures from @TheAtlantic ...   \n",
        "1469  522494785152512000  RT @krislc: Mong Kok. 100 people. but seems ba...   \n",
        "1470  522494728147726337  RT @krislc: earlier: the 1 dollar coins that c...   \n",
        "1471  522494722938388480  RT @desiree_fa: 8 Police vans roll out from wa...   \n",
        "1472  522494481552011264  Mong Kok. 100 people. but seems barricades cou...   \n",
        "1473  522493948908941312  Sign the petition: Stand in solidarity with pr...   \n",
        "1474  522493690120388608  RT @krislc: Banner now #OccupyHK http://t.co/u...   \n",
        "1475  522493482980896768  RT @krislc: .@klustout here #OccupyHK http://t...   \n",
        "1476  522493408976596993  RT @cpjasia: Five HK Press Unions United in Co...   \n",
        "1477  522493263543300096  RT @krislc: 9:11pm. Rodney Street packed #Occu...   \n",
        "1478  522493200016359424  RT @krislc: packed around the stage; but loose...   \n",
        "1479  522493095821074432   RT @krislc: now #OccupyHK http://t.co/RJ3eedsXUY   \n",
        "1480  522492944020807680  RT @AgnesBun: Meanwhile in Sheung Wan... #Occu...   \n",
        "1481  522492841759875072  RT @krislc: This is the post. started 10:30am....   \n",
        "1482  522492750152077312  RT @HKFS1958: Urgently need face masks, saline...   \n",
        "1483  522492727682809856  RT @tomgrundy: Barriers being reinforced in tu...   \n",
        "1484  522492628680839168  RT @krislc: occupied area stretches from legis...   \n",
        "1485  522492375919120384  police can't stand the verbal abuse from prote...   \n",
        "1486  522492265512845312  RT @krislc: barricades also building up westbo...   \n",
        "1487  522492212412571649  Album of close up shots of the pepper spraying...   \n",
        "1488  522492047266443264  RT @krislc: calling wooden plates here to stre...   \n",
        "1489  522492008359665665  Sign the petition: Stand in solidarity with pr...   \n",
        "1490  522492004996239361  RT @krislc: Lung Wo Rd. What? source: https://...   \n",
        "1491  522491590443806720  RT @tomgrundy: Big Pictures from @TheAtlantic ...   \n",
        "1492  522491470146969600  RT @krislc: now &amp; signing off. #OccupyHK h...   \n",
        "1493  522491449322258434  RT @krislc: this amount of people will stay at...   \n",
        "1494  522490688298967040  Sign the petition: Stand in solidarity with pr...   \n",
        "1495  522490643483217920  RT @fion_li: Hong Kong Police Battle Protester...   \n",
        "\n",
        "       hashtag  \n",
        "0     hongkong  \n",
        "1     hongkong  \n",
        "2     hongkong  \n",
        "3     hongkong  \n",
        "4     hongkong  \n",
        "5     hongkong  \n",
        "6     hongkong  \n",
        "7     hongkong  \n",
        "8     hongkong  \n",
        "9     hongkong  \n",
        "10    hongkong  \n",
        "11    hongkong  \n",
        "12    hongkong  \n",
        "13    hongkong  \n",
        "14    hongkong  \n",
        "15    hongkong  \n",
        "16    hongkong  \n",
        "17    hongkong  \n",
        "18    hongkong  \n",
        "19    hongkong  \n",
        "20    hongkong  \n",
        "21    hongkong  \n",
        "22    hongkong  \n",
        "23    hongkong  \n",
        "24    hongkong  \n",
        "25    hongkong  \n",
        "26    hongkong  \n",
        "27    hongkong  \n",
        "28    hongkong  \n",
        "29    hongkong  \n",
        "...        ...  \n",
        "1466  occupyhk  \n",
        "1467  occupyhk  \n",
        "1468  occupyhk  \n",
        "1469  occupyhk  \n",
        "1470  occupyhk  \n",
        "1471  occupyhk  \n",
        "1472  occupyhk  \n",
        "1473  occupyhk  \n",
        "1474  occupyhk  \n",
        "1475  occupyhk  \n",
        "1476  occupyhk  \n",
        "1477  occupyhk  \n",
        "1478  occupyhk  \n",
        "1479  occupyhk  \n",
        "1480  occupyhk  \n",
        "1481  occupyhk  \n",
        "1482  occupyhk  \n",
        "1483  occupyhk  \n",
        "1484  occupyhk  \n",
        "1485  occupyhk  \n",
        "1486  occupyhk  \n",
        "1487  occupyhk  \n",
        "1488  occupyhk  \n",
        "1489  occupyhk  \n",
        "1490  occupyhk  \n",
        "1491  occupyhk  \n",
        "1492  occupyhk  \n",
        "1493  occupyhk  \n",
        "1494  occupyhk  \n",
        "1495  occupyhk  \n",
        "\n",
        "[1496 rows x 3 columns]\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Finding uniqueness of tweets\n",
      "\n",
      "One important thing to do would be to find the uniqueness of a dataset. Here, we should measure uniqueness as number of unique tweets / number of tweets in the data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tweet_uniqueness(tweets):\n",
      "    # Code for finding the number of unique tweets in a column over the number of tweets.\n",
      "    # should return a number between 0 and 1\n",
      "\n",
      "# shows that with the code above, we didn't get completely unique tweets.\n",
      "print tweet_uniqueness(tweets.id) \n",
      "\n",
      "# code to drop duplicates based on the id column alone.\n",
      "unique_tweets = tweets.drop_duplicates(cols=['id'])\n",
      "\n",
      "print len(unique_tweets)\n",
      "print tweet_uniqueness(unique_tweets.id)\n",
      "print tweet_uniqueness(unique_tweets.tweet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.761363636364\n",
        "1139\n",
        "1.0\n",
        "0.779631255487\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Even by removing the duplicate rows of data, we still have a significant amount of tweets that are the same, likely due to retweets.\n",
      "\n",
      "How could we confirm this hypotheses with pandas? Based on this limited dataset, what hashtag seems to get the most retweeted, vs the least retweeted?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in unique_tweets.hashtag.unique():\n",
      "    print i, tweet_uniqueness(unique_tweets[unique_tweets.hashtag == i].tweet)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hongkong 0.863636363636\n",
        "occupycentral 0.719047619048\n",
        "umbrellarevolution 0.630872483221\n",
        "china 0.950980392157\n",
        "hk 0.970588235294\n",
        "admiralty 0.716981132075\n",
        "occupyhk 0.610778443114\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we'll use nltk to tokenize the tweets. Tokens are really just units of words within parsed text.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import nltk\n",
      "\n",
      "def tokenize_tweet(t, remove_stop=True, remove_hashtag=False):\n",
      "    import string\n",
      "    import re\n",
      "    tweet = t\n",
      "    tweet = tweet.lower()\n",
      "    tweet = re.sub('@\\w+', 'TWITTER_HANDLE', tweet)\n",
      "    tweet = re.sub('(https?:\\/\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\/\\w \\.-]*)*\\/?', 'URL', tweet)\n",
      "    tweet = tweet.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
      "    words = nltk.tokenize.wordpunct_tokenize(tweet)\n",
      "    if remove_stop:\n",
      "        # How do we filter for words in the stopwords corpus?\n",
      "        stopwords_filter = set(nltk.corpus.stopwords.words('english'))\n",
      "\n",
      "    if remove_hashtag:\n",
      "        # How do we filter out the actual hashtag in the tweet itself?\n",
      "\n",
      "    return words\n",
      "\n",
      "\n",
      "unique_tweets['tokens'] = unique_tweets.tweet.apply(tokenize_tweet, remove_stop=True)\n",
      "unique_tweets['tokens_w_stopwords'] = unique_tweets.tweet.apply(tokenize_tweet, remove_stop=False)\n",
      "\n",
      "print unique_tweets['tokens_w_stopwords']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0     [is, this, hongkong, s, rodney, king, police, ...\n",
        "1     [we, wont, move, and, im, ready, to, get, arre...\n",
        "2     [rt, TWITTERHANDLE, footage, of, beating, prom...\n",
        "3     [what, is, happening, in, hong, kong, is, some...\n",
        "4     [fundinghongkong, travel, startup, TWITTERHAND...\n",
        "5     [hk, police, use, pepper, spray, on, protester...\n",
        "6     [rt, TWITTERHANDLE, footage, of, beating, prom...\n",
        "7     [rt, TWITTERHANDLE, footage, of, beating, prom...\n",
        "8     [squeezed, vscocam, vscohub, vscogang, vscogra...\n",
        "9     [rt, TWITTERHANDLE, footage, of, beating, prom...\n",
        "10    [breaking, hongkong, security, chief, says, 6,...\n",
        "11    [trendingnews, hongkong, demonstranten, aangek...\n",
        "12    [TWITTERHANDLE, in, hongkong, umbrellamovement...\n",
        "13    [TWITTERHANDLE, in, hongkong, umbrellamovement...\n",
        "14    [map, of, the, underpass, in, hongkong, where,...\n",
        "...\n",
        "1476    [rt, TWITTERHANDLE, five, hk, press, unions, u...\n",
        "1477    [rt, TWITTERHANDLE, 911pm, rodney, street, pac...\n",
        "1478    [rt, TWITTERHANDLE, packed, around, the, stage...\n",
        "1479              [rt, TWITTERHANDLE, now, occupyhk, URL]\n",
        "1481    [rt, TWITTERHANDLE, this, is, the, post, start...\n",
        "1482    [rt, TWITTERHANDLE, urgently, need, face, mask...\n",
        "1484    [rt, TWITTERHANDLE, occupied, area, stretches,...\n",
        "1485    [police, cant, stand, the, verbal, abuse, from...\n",
        "1486    [rt, TWITTERHANDLE, barricades, also, building...\n",
        "1487    [album, of, close, up, shots, of, the, pepper,...\n",
        "1488    [rt, TWITTERHANDLE, calling, wooden, plates, h...\n",
        "1490    [rt, TWITTERHANDLE, lung, wo, rd, what, source...\n",
        "1491    [rt, TWITTERHANDLE, big, pictures, from, TWITT...\n",
        "1492    [rt, TWITTERHANDLE, now, amp, signing, off, oc...\n",
        "1493    [rt, TWITTERHANDLE, this, amount, of, people, ...\n",
        "Name: tokens_w_stopwords, Length: 1139, dtype: object\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Consider the following question to answer:\n",
      "\n",
      "Do different audiences use different hashtags surrounding the hong kong protests?\n",
      "\n",
      "We can use nltk to help break apart the words in the tweets and determine how words may change based on the hashtag.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# pos_tag is a part of speech tagger, based on the text that it ingests. \n",
      "# while its not built for twitter data, we can try it out and see how accurate it is\n",
      "nltk.pos_tag\n",
      "unique_tweets['pos'] = unique_tweets['tokens'].apply(nltk.pos_tag)\n",
      "\n",
      "# Printing out all words that come back as adjectives (JJ):\n",
      "adjectives = []\n",
      "for i in unique_tweets.pos:\n",
      "    bb = [j[0] for j in i if j[1] == 'JJ']\n",
      "    if bb:\n",
      "        adjectives.extend(bb)\n",
      "\n",
      "print list(set(adjectives))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['heutejournal', 'malicious', 'excessive', 'global', 'yellow', 'kong', 'top', 'govt', 'violent', 'asian', 'human', 'japan', 'taiwan', 'hate', 'blow', '116th', 'le', 'recipient', 'young', 'easy', 'rabble', 'cardinal', 'govern', 'te', 'real', 'good', 'animal', 'big', 'stop', 'possible', 'early', 'joint', 'traffic', 'shameful', 'anonymous', 'international', 'front', 'trouble', 'occupyhongkong', 'helpful', 'cable', 'tear', 'large', 'investigate', 'small', 'insane', 'financial', 'verbal', 'national', 'dead', 'hair', 'likely', 'economic', '3nsailing', 'agricultural', 'corrupt', 'sexual', 'special', 'viral', 'tungchung', 'symbiotic', 'bad', 'legal', 'creative', 'current', 'outside', 'various', 'new', 'adorable', 'public', '3d', 'available', 'suisheng', 'full', 'christian', 'pathetic', 'free', 'strong', 'soundcloud', 'jubilant', 'sheung', 'prodemocratic', 'great', 'bible', 'central', 'standby', 'many', 'aguus', 'equal', 'urged', 'foreign', 'protestorsoccupycentral', 'social', 'military', 'weird', 'create', 'first', 'major', 'industrial', 'vid', 'civic', 'irresponsible', 'powerful', 'arsenal', 'private', 'supporthongkong', 'lung', 'female', 'spanish', 'open', 'legislative', 'wan', 'additive', 'silent', 'ophongkong', 'angry', 'fiveyear', 'genetic', 'next', 'occupy', 'live', 'occupycentral', 'low', 'peaceful', 'australian', '3dprinting', 'website', 'british', 'gear', 'riot', 'successful', 'basic', 'alive', 'icable', 'last', 'general', 'former', 'main', 'dangerous', 'outrageous', 'sous', 'future', 'obvious', 'dirty', 'ive', 'gratuitous', 'chaotic', 'pic', 'site', 'overzealous', 'comfortable', 'high', 'middle', 'brutal', 'want', 'ready', 'wonderful', 'david', 'occupied', 'eric', 'physical', 'opportunityfinancial', 'kush', 'hong', 'hongkong', 'mic', 'responsible', 'hot', 'republic', 'fourth', 'automotive', 'widespread', 'several', 'beach', 'poor', 'independent', 'finish', 'normal', 'impartial', 'english', 'evil', 'sigue', 'wrong', 'serious', 'recent', 'natural', 'third', 'clear', 'proud', 'acknowledged', 'thought', 'concrete', 'tech', 'african', 'professional', 'democratic', 'microsoft', 'unspeakable']\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What are some of the challenges that this tagger seems to face?\n",
      "\n",
      "Above it's clear it does... okay. Since pos_tag trains based on its input, we could theoretically from here create a corpus that makes up for the expected English patterns, and then it will be better calibrated. For today, let's consider a smaller list of adjectives based on what nltk believes it found for adjectives.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "adjective_list = {\n",
      "    'industrial': 0,\n",
      "    'excessive': -1,\n",
      "    'gratuitous': 1,\n",
      "    'chaotic': -1,\n",
      "    'national': 1,\n",
      "    'young': 1,\n",
      "    'yellow': 0,\n",
      "    'high': 1,\n",
      "    'middle': 0,\n",
      "    'likely': 1,\n",
      "    'economic': 0,\n",
      "    'creative': 1,\n",
      "    'open': 1,\n",
      "    'physical': 0,\n",
      "    'symbiotic': 1,\n",
      "    'legal': 1,\n",
      "    'next': 1,\n",
      "    'genetic': 0,\n",
      "    'angry': -1,\n",
      "    'strong': 1,\n",
      "    'peaceful': 1,\n",
      "    'new': 1,\n",
      "    'widespread': 1,\n",
      "    'real': 1,\n",
      "    'good': 1,\n",
      "    'normal': 0,\n",
      "    'successful': 1,\n",
      "    'big': 1,\n",
      "    'basic': -1,\n",
      "    'hate': -1,\n",
      "    'private': -1,\n",
      "    'front': 0,\n",
      "    'central': 0,\n",
      "    'comfortable': 1,\n",
      "    'last': 0,\n",
      "    'helpful': 1,\n",
      "    'third': 0,\n",
      "    'many': 1,\n",
      "    'clear': 1,\n",
      "    'proud': 1,\n",
      "    'brutal': -1,\n",
      "    'large': 1,\n",
      "    'dirty': -1,\n",
      "    'professional': 1,\n",
      "    'first': 0,\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Goal: Using the adjectives and some measure of sentiment, predict a given hashtag?**\n",
      "\n",
      "We'll need to build a couple more functions:\n",
      "\n",
      "1. Let's write a function that creates a sentiment score for each tweet based on the adjectives above.\n",
      "2. Set the targets of the model. We can do this two ways:\n",
      "    1. One, using the `hashtag` column, which is what was generated based on the twitter search\n",
      "    2. Create several target columns, based on the tweet itself, using a regex match for the hashtag.\n",
      "3. Finally, build a logistic regression using NLTK's sklearn implementation using our created sentiment as a regressor."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# First function: create a sentiment score column.\n",
      "# Should take in a list of words, and return back the score as\n",
      "# mean(sentiment_of_adjectives)\n",
      "def measure_sentiment(words):\n",
      "\n",
      "\n",
      "# Second function: Create a numeric target column.\n",
      "def numeric_hashtag(tag):\n",
      "    # we could use a dictionary similar to above to easily map hashtags to numeric values.\n",
      "    targets = {\n",
      "        u'hongkong': 0,\n",
      "        u'occupycentral': 1,\n",
      "        u'umbrellarevolution':2,\n",
      "        u'china':3,\n",
      "        u'hk':4,\n",
      "        u'admiralty':5,\n",
      "        u'occupyhk':6,\n",
      "    }\n",
      "    return targets[tag]\n",
      "\n",
      "unique_tweets['sentiment'] = unique_tweets.tokens.apply(measure_sentiment)\n",
      "print unique_tweets.sentiment.hist()\n",
      "\n",
      "unique_tweets['target'] = unique_tweets.hashtag.apply(numeric_hashtag)\n",
      "\n",
      "# Build a logistic regression using the sentiment feature and the numeric hashtags\n",
      "from sklearn import linear_model as lm\n",
      "\n",
      "lmfit = lm.LogisticRegression()\n",
      "lmfit.fit(unique_tweets[['sentiment']], unique_tweets['target'])\n",
      "print lmfit.score(unique_tweets[['sentiment']], unique_tweets['target'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Axes(0.125,0.125;0.775x0.775)\n",
        "0.197541703248"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEACAYAAABbMHZzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNn9JmkI2f0maQv8NiaD3qfuWJKoAAAAASUVORK5CYII=\n",
       "text": [
        "<matplotlib.figure.Figure at 0x112e35650>"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Some things we didn't talk about\n",
      "\n",
      "A common approach to handling text data is working with term frequencies and inverse document frequencies. sk-learn actually has a TF-IDF vectorizer that would be useful to interact with. A TF vectorizer would also be a fine start when working with larger texts.\n",
      "\n",
      "These matrices (TF and TF-IDF) are also strong indicators for classification.\n",
      "Consider the usage of stopwords. Stopswords can actually be incredibly useful in predicting! (http://rforwork.info/2012/12/27/intro-to-mult-classification/)\n",
      "\n",
      "## Next steps/Homework\n",
      "Continue iterating on the twitter data set. What new features could you curate that could be predictive of a hashtag?\n",
      "\n",
      "Consider:\n",
      "\n",
      "1. length of a tweet, or number of tokens in a tweet\n",
      "2. Finding more words to include in the sentiment dictionary\n",
      "3. Finding words most commonly used along a hashtag.\n",
      "\n",
      "add to the code, and build a better model using your new features.\n",
      "\n",
      "**OR**\n",
      "\n",
      "follow a different persuit with the data, given that you can work with the twitter api, which will provide you with much more data than included in the master csv file here.\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}